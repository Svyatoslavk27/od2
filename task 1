import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# --- 1. ЗАВАНТАЖЕННЯ ДАНИХ ---
file_path = r'C:\Users\LOQ\Downloads\Telegram Desktop\A14.txt'
try:
    # Завантаження без заголовка
    df = pd.read_csv(file_path, header=None)
    print(f"Дані завантажено. Розмірність: {df.shape}")
except FileNotFoundError:
    # Тестові дані, якщо файл не знайдено
    print("Файл не знайдено, генеруємо тестові дані...")
    t = np.linspace(0, 10, 5000)
    df = pd.DataFrame(np.sin(t[:, None] + np.random.randn(1, 12)))

# Параметри з умови
N = df.shape[0]
time = np.linspace(0, 10, N)

# --- 2. ПОПЕРЕДНЯ ОБРОБКА (Стандартизація) ---
# Для PCA важливо, щоб дані мали середнє=0 і дисперсію=1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# --- 3. ФАКТОРНИЙ АНАЛІЗ (PCA) ---
# Знаходимо всі компоненти (12 штук)
pca = PCA(n_components=12)
X_pca = pca.fit_transform(X_scaled)

# Власні числа (Eigenvalues) - пояснена дисперсія
eigenvalues = pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_ * 100
cumulative_variance = np.cumsum(explained_variance_ratio)

print("\n--- Результати Факторного Аналізу ---")
print(f"{'№':<5} {'Власне число':<15} {'% Дисперсії':<15} {'Сумарна %':<15}")
for i in range(12):
    print(f"{i+1:<5} {eigenvalues[i]:<15.4f} {explained_variance_ratio[i]:<15.2f} {cumulative_variance[i]:<15.2f}")

# Критерій інформативності для перших 3-х компонент (як у прикладі методички)
info_criterion = sum(eigenvalues[:3]) / sum(eigenvalues)
print(f"\nКритерій інформативності (k=3): {info_criterion:.4f} (пояснює {cumulative_variance[2]:.2f}% дисперсії)")

# --- Графік "Кам'янистий осип" (Scree Plot) [Рис. 7 методички] ---
plt.figure(figsize=(10, 5))
plt.plot(range(1, 13), eigenvalues, 'bo-', linewidth=2)
plt.title("Графік кам'янистого осипу (Scree Plot)")
plt.xlabel("Номер компоненти")
plt.ylabel("Власне число")
plt.grid(True)
plt.show()

# --- Графік першої головної компоненти (Z1) [Рис. 10 методички] ---
plt.figure(figsize=(12, 4))
plt.plot(time, X_pca[:, 0], color='purple')
plt.title("Перша головна компонента (Z1)")
plt.xlabel("Час (сек)")
plt.grid(True)
plt.show()

# --- 4. КЛАСТЕРНИЙ АНАЛІЗ (K-Means) ---
# Варіант А: Кластеризація вихідних даних (12D)
# Варіант Б: Кластеризація головних факторів (3D - Z1, Z2, Z3)
# Виконаємо для 3D, як найбільш наочний варіант, з k=7 (як на Рис. 12)

k_clusters = 7
kmeans = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)
# Беремо перші 3 компоненти
X_3d = X_pca[:, :3] 
clusters = kmeans.fit_predict(X_3d)

# Додаємо мітки кластерів до даних
df_res = pd.DataFrame(X_3d, columns=['Z1', 'Z2', 'Z3'])
df_res['Cluster'] = clusters

# --- Візуалізація Кластерів (Проекція на Z1-Z2) ---
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df_res, x='Z1', y='Z2', hue='Cluster', palette='viridis', s=50, alpha=0.6)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            s=200, c='red', marker='X', label='Центри')
plt.title(f"Результати кластеризації K-Means (k={k_clusters}) у просторі PCA")
plt.xlabel("Головна компонента 1 (Z1)")
plt.ylabel("Головна компонента 2 (Z2)")
plt.legend()
plt.grid(True)
plt.show()

# --- Візуалізація R-піків на основі кластерів ---
# Зазвичай один з кластерів відповідає за R-піки (максимальні відхилення)
# Побудуємо графік Z1, розфарбований по кластерах
plt.figure(figsize=(15, 5))
plt.scatter(time, df_res['Z1'], c=df_res['Cluster'], cmap='viridis', s=10)
plt.title("Сигнал Z1 з кольоровим кодуванням кластерів")
plt.xlabel("Час")
plt.ylabel("Амплітуда Z1")
plt.colorbar(label='Номер кластера')
plt.show()
